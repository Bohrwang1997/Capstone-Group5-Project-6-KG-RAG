{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import necesssary tookits\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NI02-4DRLa_E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the file\n",
        "def readdef(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df['Definitions'].tolist()\n",
        "\n",
        "definitions = readdef('ctx_pd.csv')"
      ],
      "metadata": {
        "id": "_xBIZqqHLe9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create T5 model\n",
        "class definitionassistant:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
        "\n",
        "    def defprompt(self, query, definitions):\n",
        "        # put the definition into the prompt\n",
        "        context = \" \".join(definitions)\n",
        "        prompt = f\"\"\"Answer the following question based on the provided context.\n",
        "        Question: {query}\n",
        "        Context: {context}\n",
        "        Output:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def reply(self, query, definitions):\n",
        "        # combine the search result and the generalized sentence\n",
        "        prompt = self.defprompt(query, definitions)\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)  # 移动输入到GPU\n",
        "        outputs = self.model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
        "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return answer"
      ],
      "metadata": {
        "id": "fDnFPyeAMJTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model\n",
        "assistant = definitionassistant()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "f67iHmbVMqg6",
        "outputId": "53516b0c-f20e-48ee-f709-b204b8e4f65d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'definitionassistant' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-16dda88f86dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massistant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefinitionassistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'definitionassistant' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing samples\n",
        "query = \"What is accreditation?\"\n",
        "reply = assistant.reply(query, definitions)\n",
        "print(f\"Query: {query}\\nGenerated Reply:\\n{reply}\")"
      ],
      "metadata": {
        "id": "3vRhWRNFM2Tj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}